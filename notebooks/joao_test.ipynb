{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if 'Conformal-Sparsemax/notebooks' in os.getcwd():\n",
    "    os.chdir(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.sort()\n",
    "# x = x[0,::-1]\n",
    "# beta = 0.1\n",
    "# for k in range(len(x)):\n",
    "#     if 1+k*x[k]*beta <= np.sum(x[:k]*beta):\n",
    "#         break\n",
    "# (np.sum(x[:k]) - 1)/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_score(x):\n",
    "    orig_x = x.copy()\n",
    "    output = np.zeros(x.shape)\n",
    "    for k in range(orig_x.shape[1]):\n",
    "        for i in range(orig_x.shape[0]):\n",
    "            rank = int(orig_x.shape[1]-ss.rankdata(orig_x[i])[k])\n",
    "            x = orig_x[i].copy()\n",
    "            x.sort()\n",
    "            x = x[::-1]\n",
    "            output[i,k] = (np.sum(x[:rank]))/(rank+0.001) - x[rank]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get non-conformity scores\n",
    "alpha = 0.1\n",
    "pred_cal_path = 'predictions/CIFAR10_cal_FYLoss_logits_proba.pickle'\n",
    "pred_test_path = 'predictions/CIFAR10_test_FYLoss_logits_proba.pickle'\n",
    "true_cal_path = 'predictions/CIFAR10_cal_true.pickle'\n",
    "true_test_path = 'predictions/CIFAR10_test_true.pickle'\n",
    "\n",
    "def get_data(pred_cal_path, pred_test_path,true_cal_path, true_test_path):\n",
    "    with open(pred_cal_path, 'rb') as f:\n",
    "        pred_cal = pickle.load(f)\n",
    "    with open(pred_test_path, 'rb') as f:\n",
    "        pred_test = pickle.load(f)\n",
    "    with open(true_cal_path, 'rb') as f:\n",
    "        true_cal = pickle.load(f)\n",
    "    with open(true_test_path, 'rb') as f:\n",
    "        true_test = pickle.load(f)\n",
    "    return pred_cal, pred_test, true_cal, true_test\n",
    "    \n",
    "def run_cp(pred_cal, pred_test, true_cal, true_test, alpha, plots = False):\n",
    "    def get_pvalue(preds):\n",
    "            return np.array([((cal_scores>= el).sum() + 1)/(len(cal_scores) + 1) for el in preds])\n",
    "    \n",
    "    #def cp_classifier(pred_cal, true_cal, pred_test, true_test, method='naive',alpha=0.1):\n",
    "    n_cal, n_classes = pred_cal.shape \n",
    "    n_test = true_test.shape[0]\n",
    "    #print(f'{n_cal} calibration points')\n",
    "    #print(f'{n_test} test points')\n",
    "    #print(f'{n_classes} classes')\n",
    "    \n",
    "    # Get calibration quantile\n",
    "    true_mask = true_cal.astype(bool)\n",
    "    # cal_scores = 1 - pred_cal[true_mask]\n",
    "    cal_scores = custom_score(pred_cal)\n",
    "    #cal_scores = ((1 - pred_cal[true_mask])/(n_classes-pred_cal.astype(bool).sum(axis=1).reshape((n_cal,1)).T)).T\n",
    "    q_level = np.ceil((n_cal+1)*(1-alpha))/n_cal\n",
    "    qhat = np.quantile(cal_scores, q_level) # check quantile method\n",
    "    \n",
    "    # test predictions\n",
    "    # test_scores = 1 - pred_test\n",
    "    test_scores = custom_score(pred_test)\n",
    "    #alternative\n",
    "    #test_scores = ((1 - pred_test)/(n_classes-pred_test.astype(bool).sum(axis=1).reshape((n_test,1))))\n",
    "    test_match = test_scores<= qhat\n",
    "    # get p-values \n",
    "    test_pvalues = np.apply_along_axis(get_pvalue,1,test_scores)\n",
    "    p_values_cal = get_pvalue(cal_scores)\n",
    "    \n",
    "    # Set size and scores distribution\n",
    "    set_size = test_match.sum(axis = 1)\n",
    "    if plots:   \n",
    "        fig, axs = plt.subplots(1,2,figsize=(12,6))\n",
    "        axs[0].hist(set_size)\n",
    "        axs[0].vlines(set_size.mean(),0,max(np.histogram(set_size, bins=10)[0])+10, color='black')\n",
    "        axs[0].text(set_size.mean()*1.02,max(np.histogram(set_size, bins=10)[0]-10)*0.95,  f'S = {set_size.mean()}', color='black',fontweight='bold')\n",
    "        axs[0].set_title('Set Size Distribution')\n",
    "        \n",
    "        axs[1].hist(cal_scores)\n",
    "        axs[1].vlines(qhat,0,max(np.histogram(cal_scores, bins=10)[0])+10, color='black')\n",
    "        axs[1].text(qhat*1.02,max(np.histogram(cal_scores, bins=10)[0]-10)*0.95, f'q={qhat:.3f}', color='black',fontweight='bold')\n",
    "        axs[1].set_title('Non-Conf Scores Distribution')\n",
    "        plt.show()\n",
    "    coverage = test_match[true_test.astype(bool)].sum()/n_test\n",
    "    #print(f'Coverage:{coverage}')\n",
    "    class_coverage = (test_match & true_test).sum(axis = 0)/true_test.sum(axis=0)\n",
    "    \n",
    "    set_size = test_match.sum(axis = 1)\n",
    "    #print(f'Avg set size:{set_size.mean()}')\n",
    "    class_size = true_test.copy()\n",
    "    class_size[class_size==1]=test_match.sum(axis = 1)\n",
    "    class_size = class_size.sum(axis=0)/true_test.sum(axis=0)\n",
    "\n",
    "    if plots:\n",
    "    # Class-wise metrics\n",
    "        fig, axs = plt.subplots(1,2,figsize=(12,6))\n",
    "        # add labels?\n",
    "        axs[0].bar(np.arange(n_classes),class_coverage)\n",
    "        axs[0].hlines(coverage,0,n_classes-1, color='black')\n",
    "        axs[0].hlines(1-alpha,0,n_classes-1, color='green')\n",
    "        axs[0].text(0,coverage, f'Emp. cov. = {coverage:.2f}', color='black',fontweight='bold')\n",
    "        axs[0].text(0,1-alpha, f'Theo. cov. = {1-alpha:.2f}', color='green',fontweight='bold')\n",
    "        axs[0].set_title('Class Conditional Coverage')\n",
    "        \n",
    "        \n",
    "        axs[1].bar(np.arange(n_classes),class_size)\n",
    "        axs[1].hlines(set_size.mean(),0,100, color='black')\n",
    "        axs[1].text(0,set_size.mean(), f'S={set_size.mean():.3f}', color='black',fontweight='bold')\n",
    "        axs[1].set_title('Class Avg Set size')\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    # Observed fuzziness\n",
    "    of = np.ma.array(test_pvalues, mask = true_test).mean(axis=1).data.mean()\n",
    "    #print(f'OF={of:.4f}')\n",
    "    return test_match, coverage, set_size.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cal, pred_test, true_cal, true_test = get_data(pred_cal_path, pred_test_path,true_cal_path, true_test_path)\n",
    "alphas = np.linspace(0.001,0.4,20)\n",
    "test_matches_custom = []\n",
    "coverages_custom = []\n",
    "avg_sizes_custom = []\n",
    "for alpha in alphas:\n",
    "    test_match, coverage, avg_size = run_cp(pred_cal, pred_test, true_cal, true_test, alpha, plots = False)\n",
    "    test_matches_custom.append(test_match)\n",
    "    coverages_custom.append(coverage)\n",
    "    avg_sizes_custom.append(avg_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
